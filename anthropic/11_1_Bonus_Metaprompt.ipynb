{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1mtyzbvpM2L"
   },
   "source": [
    "# Metaprompt\n",
    "Welcome to the Metaprompt! This is a prompt engineering tool designed to solve the \"blank page problem\" and give you a starting point for iteration. All you need to do is enter your task, and optionally the names of the variables you'd like Claude to use in the template. Then you'll be able to run the prompt that comes out on any examples you like.\n",
    "\n",
    "**Caveats**\n",
    "- This is designed for single-turn question/response prompts, not multiturn.\n",
    "- The Metaprompt is designed for use with **Claude 3 Opus.** Generating prompts with other models may lead to worse results.\n",
    "- The prompt you'll get at the end is not guaranteed to be optimal by any means, so don't be afraid to change it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOYnM7A3g6HB"
   },
   "source": [
    "### Using This Notebook\n",
    "The notebook is designed to be maximally easy to use. You don't have to write any code. Just follow these steps:\n",
    "- Enter your Anthropic API key in between quotation marks where it says \"Put your API key here!\"\n",
    "- Enter your task where it says \"Replace with your task!\"\n",
    "- Optionally, enter an all-caps list of variables in quotes separated by commas where it says \"specify the input variables you want Claude to use\".\n",
    "\n",
    "Then, you can simply click \"Runtime -> Run all\" and your prompt will be displayed at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic --quiet\n",
    "\n",
    "# set path to import the meta module that contains the metaprompt\n",
    "import os\n",
    "import sys\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "import boto3\n",
    "session = boto3.Session() # create a boto3 session to dynamically get and set the region name\n",
    "AWS_REGION = session.region_name\n",
    "print(\"AWS Region:\", AWS_REGION)\n",
    "\n",
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "\n",
    "from anthropic import AnthropicBedrock\n",
    "CLIENT = AnthropicBedrock(aws_region=AWS_REGION)\n",
    "MODEL_NAME='anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "%store MODEL_NAME\n",
    "%store AWS_REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ-pJY_ZqtGE"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "0. [The Metaprompt](#0-the-metaprompt)\n",
    "1. [Quickstart](#1-quickstart) - Enter a task, get a prompt template\n",
    "2. [Testing your prompt template](#2-testing-your-prompt-template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBbTQMwMqcUT"
   },
   "source": [
    "## 0. The Metaprompt\n",
    "\n",
    "The Metaprompt is a long multi-shot prompt filled with half a dozen examples of good prompts for solving various tasks. These examples help Claude to write a good prompt for your task. The full text is below (warning: it's long!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NTOiFKNxqoq2"
   },
   "outputs": [],
   "source": [
    "# import the meta module from the utils package\n",
    "# see utils/metap.py for the implementation\n",
    "# Import the metap module from the utils package\n",
    "from utils import meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RGDChQBsgKa"
   },
   "source": [
    "# 1. Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StAFtG7Cskn2"
   },
   "source": [
    "Enter your task in the cell below. Here are some examples for inspiration:\n",
    "- Choose an item from a menu for me given user preferences\n",
    "- Rate a resume according to a rubric\n",
    "- Explain a complex scientific concept in simple terms\n",
    "- Draft an email responding to a customer complaint\n",
    "- Design a marketing strategy for launching a new product\n",
    "\n",
    "There are two examples of tasks + optional variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPySubcpKiwg"
   },
   "outputs": [],
   "source": [
    "TASK = \"Draft an email responding to a customer complaint\" # Replace with your task!\n",
    "# Optional: specify the input variables you want Claude to use. If you want Claude to choose, you can set `variables` to an empty list!\n",
    "# VARIABLES = []\n",
    "VARIABLES = [\"CUSTOMER_EMAIL\", \"COMPANY_NAME\"]\n",
    "# If you want Claude to choose the variables, just leave VARIABLES as an empty list.\n",
    "\n",
    "# TASK = \"Choose an item from a menu for me given my preferences\"\n",
    "# VARIABLES = []\n",
    "# VARIABLES = [\"MENU\", \"PREFERENCES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKxOMb4Wrh9T",
    "outputId": "9247da39-aa8a-484e-e22b-36b956d81641"
   },
   "outputs": [],
   "source": [
    "variable_string = \"\"\n",
    "for variable in VARIABLES:\n",
    "    variable_string += \"\\n{\" + variable.upper() + \"}\"\n",
    "print(variable_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIUwWJwfs_mp"
   },
   "source": [
    "Next, we'll insert your task into the metaprompt and see what Claude gives us! Expect this to take 20-30 seconds because the Metaprompt is so long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihxetJAns9fo"
   },
   "outputs": [],
   "source": [
    "prompt = meta.metaprompt.replace(\"{{TASK}}\", TASK)\n",
    "assistant_partial = \"<Inputs>\"\n",
    "if variable_string:\n",
    "    assistant_partial += variable_string + \"\\n</Inputs><Instructions Structure>\"\n",
    "\n",
    "message = CLIENT.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":  prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_partial\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ").content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzuwEYDQCdq9"
   },
   "source": [
    "If you want to see the full text returned by the Metaprompt to see how it planned things out, uncomment out the \"pretty_print(message)\" line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlaPmC6QCcYO"
   },
   "outputs": [],
   "source": [
    "def pretty_print(message):\n",
    "    print('\\n\\n'.join('\\n'.join(line.strip() for line in re.findall(r'.{1,100}(?:\\s+|$)', paragraph.strip('\\n'))) for paragraph in re.split(r'\\n\\n+', message)))\n",
    "# pretty_print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HddM0hwBChTu"
   },
   "source": [
    "Now, we'll extract the prompt itself and the variables needed, while also removing empty tags at the end of the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIBhAFs4BFIA"
   },
   "outputs": [],
   "source": [
    "def extract_between_tags(tag: str, string: str, strip: bool = False) -> list[str]:\n",
    "    ext_list = re.findall(f\"<{tag}>(.+?)</{tag}>\", string, re.DOTALL)\n",
    "    if strip:\n",
    "        ext_list = [e.strip() for e in ext_list]\n",
    "    return ext_list\n",
    "\n",
    "def remove_empty_tags(text):\n",
    "    return re.sub(r'<(\\w+)></\\1>$', '', text)\n",
    "\n",
    "def extract_prompt(metaprompt_response):\n",
    "    between_tags = extract_between_tags(\"Instructions\", metaprompt_response)[0]\n",
    "    return remove_empty_tags(remove_empty_tags(between_tags).strip()).strip()\n",
    "\n",
    "def extract_variables(prompt):\n",
    "    pattern = r'{([^}]+)}'\n",
    "    variables = re.findall(pattern, prompt)\n",
    "    return set(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plutfqqfQzdB"
   },
   "source": [
    "Below: the variables Claude chose (if you didn't provide any; if you did, these should just be the same ones you provided), and the prompt it wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPCY1eanBFpX"
   },
   "outputs": [],
   "source": [
    "extracted_prompt_template = extract_prompt(message)\n",
    "variables = extract_variables(message)\n",
    "\n",
    "print(\"Variables:\\n\\n\" + str(variables))\n",
    "print(\"\\n************************\\n\")\n",
    "print(\"Prompt:\")\n",
    "pretty_print(extracted_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gac-QSTZLOKP"
   },
   "source": [
    "# 2. Testing your prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4x_S5sKDIKL"
   },
   "source": [
    "If you like your prompt, try it out! The cell will prompt you to add values for each variable. Then, it will be sent to Claude and you'll see Claude's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxIptUr5aZ6J"
   },
   "outputs": [],
   "source": [
    "variable_values = {}\n",
    "for variable in variables:\n",
    "    print(\"Enter value for variable:\", variable)\n",
    "    variable_values[variable] = input()\n",
    "\n",
    "prompt_with_variables = extracted_prompt_template\n",
    "for variable in variable_values:\n",
    "    prompt_with_variables = prompt_with_variables.replace(\"{\" + variable + \"}\", variable_values[variable])\n",
    "\n",
    "message = CLIENT.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":  prompt_with_variables\n",
    "        },\n",
    "    ],\n",
    ").content[0].text\n",
    "\n",
    "print(\"Claude's output on your prompt:\\n\\n\")\n",
    "pretty_print(message)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
